{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of pix2pix.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "-ymC2jSTLuj9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheDraguun/collabtf2.0/blob/master/Copy_of_pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_xnMOsbqHz61"
      },
      "cell_type": "markdown",
      "source": [
        "# Pix2Pix"
      ]
    },
    {
      "metadata": {
        "id": "-ymC2jSTLuj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Intro\n"
      ]
    },
    {
      "metadata": {
        "id": "OhgDIiroMYUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ds4o1h4WHz9U"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/generative/pix2pix\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/pix2pix.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/pix2pix.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ITZuApL56Mny"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates image to image translation using conditional GAN's, as described in [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004). Using this technique we can colorize black and white photos, convert google maps to google earth, etc. Here, we convert building facades to real buildings.\n",
        "\n",
        "In example, we will use the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/), helpfully provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/). To keep our example short, we will use a preprocessed [copy](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/) of this dataset, created by the authors of the [paper](https://arxiv.org/abs/1611.07004) above.\n",
        "\n",
        "Each epoch takes around 58 seconds on a single P100 GPU.\n",
        "\n",
        "Below is the output generated after training the model for 200 epochs.\n",
        "\n",
        "![sample output_1](https://www.tensorflow.org/images/gan/pix2pix_1.png)\n",
        "![sample output_2](https://www.tensorflow.org/images/gan/pix2pix_2.png)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "e1_Y75QXJS6h"
      },
      "cell_type": "markdown",
      "source": [
        "## Import TensorFlow and other libraries"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YfIk2es3hJEd",
        "outputId": "ce95b17e-f878-4b5a-a50d-390cc6da38f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "#!pip install tensorflow==1.12\n",
        "\n",
        "!pip show tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#from tensorflow.core.protobuf import config_pb2\n",
        "#tf.enable_eager_execution(config=config_pb2.ConfigProto(log_device_placement=True))\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from pprint import pprint\n",
        "\n",
        "print(\"Eager:\", tf.executing_eagerly())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.13.1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: opensource@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: astor, keras-preprocessing, gast, tensorboard, absl-py, tensorflow-estimator, protobuf, termcolor, numpy, wheel, grpcio, six, keras-applications\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n",
            "Eager: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6RshEoc2Ml6e",
        "colab_type": "code",
        "outputId": "9647ebb2-03e7-4527-e86d-aba5bc612758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint(devices)\n",
        "  \n",
        "  use_tpu = True\n",
        "  \n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "#generator = tf.contrib.tpu.keras_to_tpu_model(generator_,\n",
        "#    strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Not connected to a TPU runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qg0y_0-IMkbr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import functools\n",
        "def eager(func):\n",
        "    @functools.wraps(func)\n",
        "    def eager_func(*args, **kwargs):\n",
        "      #with tf.Session() as sess:\n",
        "      #   sess.run(tf.contrib.eager.py_func(func, inp=list(kwargs.values()), Tout=[]))\n",
        "      print(\"-\")\n",
        "    return eager_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjL86fFqMkRO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "iYn4MdZnKCey"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the dataset\n",
        "\n",
        "You can download this dataset and similar datasets from [here](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets). As mentioned in the [paper](https://arxiv.org/abs/1611.07004) we apply random jittering and mirroring to the training dataset.\n",
        "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`\n",
        "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Kn-k8kTXuAlv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
        "                                      origin=_URL, \n",
        "                                      extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2CbTEt448b4R",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 400\n",
        "BATCH_SIZE = 8\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aO9ZAGH5K3SY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load(image_file):\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "\n",
        "  w = tf.shape(image)[1]\n",
        "\n",
        "  w = w // 2\n",
        "  real_image = image[:, :w, :]\n",
        "  input_image = image[:, w:, :]\n",
        "\n",
        "  input_image = tf.cast(input_image, tf.float32)\n",
        "  real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4OLHMpsQ5aOv",
        "outputId": "afcc5307-5eb5-4a4a-db80-c3dd5beb3ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "@eager\n",
        "def test_load():\n",
        "  global inp, re\n",
        "  inp, re = load(PATH+'train/100.jpg')\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(inp/255.0)\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(re/255.0)\n",
        "  plt.axis('off')\n",
        "\n",
        "test_load()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rwwYQpu9FzDu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "  input_image = tf.image.resize_images(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  real_image = tf.image.resize_images(real_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  \n",
        "  return input_image, real_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Yn3IwqhiIszt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_crop(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  cropped_image = tf.image.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "  \n",
        "  return cropped_image[0], cropped_image[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "muhR2cgbLKWW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "\n",
        "def normalize(input_image, real_image):\n",
        "  input_image = (input_image / 127.5) - 1\n",
        "  real_image = (real_image / 127.5) - 1\n",
        "  \n",
        "  return input_image, real_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fVQOjcPVLrUc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@tf.function()\n",
        "def random_jitter(input_image, real_image):\n",
        "  # resizing to 286 x 286 x 3\n",
        "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
        "\n",
        "  # randomly cropping to 256 x 256 x 3\n",
        "  input_image, real_image = random_crop(input_image, real_image)\n",
        "\n",
        "  # random mirroring\n",
        "  condition = tf.random.uniform(()) > 0.5\n",
        "  # if (condition):\n",
        "  #  input_image = tf.image.flip_left_right(input_image)\n",
        "  #  real_image = tf.image.flip_left_right(real_image)\n",
        "  input_image = tf.cond(condition, lambda: input_image, lambda: tf.image.flip_left_right(input_image)) \n",
        "  real_image = tf.cond(condition, lambda: real_image, lambda: tf.image.flip_left_right(real_image))   \n",
        "    \n",
        "  return input_image, real_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "n0OGdi6D92kM",
        "outputId": "4ea921c4-ee39-4f2f-a4bc-f26db2be8431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# As you can see in the images below\n",
        "# that they are going through random jittering\n",
        "# Random jittering as described in the paper is to\n",
        "# 1. Resize an image to bigger height and width\n",
        "# 2. Randomnly crop to the original size\n",
        "# 3. Randomnly flip the image horizontally \n",
        "\n",
        "\n",
        "@eager\n",
        "def test_jitter():\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  for i in range(4):\n",
        "    rj_inp, rj_re = random_jitter(inp, re)  \n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.imshow(rj_inp/255.0)\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "  \n",
        "test_jitter()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tyaP4hLJ8b4W",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image_train(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = random_jitter(input_image, real_image)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VB3Z6D_zKSru",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image_test(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = resize(input_image, real_image, IMG_HEIGHT, IMG_WIDTH) \n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "  \n",
        "  return input_image, real_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PIGN6ouoQxt3"
      },
      "cell_type": "markdown",
      "source": [
        "## Input Pipeline"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SQHmYSmk8b4b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_train_dataset():\n",
        "  train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
        "  train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "  train_dataset = train_dataset.map(load_image_train) #,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "  train_dataset = train_dataset.repeat()\n",
        "  return train_dataset;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MS9J0yA58b4g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_test_dataset():\n",
        "  test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
        "  # shuffling so that for every epoch a different image is generated \n",
        "  # to predict and display the progress of our model.\n",
        "  test_dataset = test_dataset.shuffle(BUFFER_SIZE)\n",
        "  test_dataset = test_dataset.map(load_image_test)\n",
        "  test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "  test_dataset = test_dataset.repeat()\n",
        "  return test_dataset;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "THkoe1jNGOSO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "THY-sZMiQ4UV"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the Generator\n",
        "  * The architecture of generator is a modified U-Net.\n",
        "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
        "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n",
        "  * There are skip connections between the encoder and decoder (as in U-Net).\n",
        " \n",
        "    "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tqqvWxlw8b4l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "OUTPUT_CHANNELS = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3R09ATE_SH9P",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "    \n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "a6_uCZCppTh7",
        "outputId": "9214f961-ba34-4de4-ef93-114fbc3d15b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "@eager\n",
        "def test_down():\n",
        "  global down_result\n",
        "  down_model = downsample(3, 4)\n",
        "  down_result = down_model(tf.expand_dims(inp, 0))\n",
        "  print (down_result.shape)\n",
        "  \n",
        "test_down()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nhgDsHClSQzP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same', \n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "  \n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mz-ahSdsq0Oc",
        "outputId": "6c8d6437-d383-40b0-9518-758348ef6a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "@eager\n",
        "def test_up():\n",
        "  global down_result\n",
        "  up_model = upsample(3, 4)\n",
        "  up_result = up_model(down_result)\n",
        "  print (up_result.shape)\n",
        "  \n",
        "test_up()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lFPI4Nu-8b4q",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Generator(name=\"Generator\"):\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "    downsample(128, 4), # (bs, 64, 64, 128)\n",
        "    downsample(256, 4), # (bs, 32, 32, 256)\n",
        "    downsample(512, 4), # (bs, 16, 16, 512)\n",
        "    downsample(512, 4), # (bs, 8, 8, 512)\n",
        "    downsample(512, 4), # (bs, 4, 4, 512)\n",
        "    downsample(512, 4), # (bs, 2, 2, 512)\n",
        "    downsample(512, 4), # (bs, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "    upsample(256, 4), # (bs, 32, 32, 512)\n",
        "    upsample(128, 4), # (bs, 64, 64, 256)\n",
        "    upsample(64, 4), # (bs, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4, \n",
        "                                         strides=2, \n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "  concat = tf.keras.layers.Concatenate() \n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=[IMG_WIDTH,IMG_HEIGHT,3], batch_size=BATCH_SIZE, name=\"generator_condition_input\")\n",
        "  x = inputs\n",
        "  \n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    #x = concat([x, skip])\n",
        "    x = tf.keras.layers.concatenate([x, skip]) \n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x , name=name )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U1N1_obwtdQH",
        "outputId": "432617a4-aed1-4c35-8311-6937fb38922d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "@eager\n",
        "def test_generator():\n",
        "  global gen_output\n",
        "  generator = Generator(name=\"G\")\n",
        "  generator.summary()\n",
        "  gen_output = generator(inp[tf.newaxis,...], training=False)\n",
        "  plt.imshow(gen_output[0,...])\n",
        "  plt.axis('off')\n",
        "  \n",
        "test_generator()  "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZTKZfoaoEF22"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the Discriminator\n",
        "  * The Discriminator is a PatchGAN.\n",
        "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
        "  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n",
        "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
        "  * Discriminator receives 2 inputs.\n",
        "    * Input image and the target image, which it should classify as real.\n",
        "    * Input image and the generated image (output of generator), which it should classify as fake. \n",
        "    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ll6aNeQx8b4v",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Discriminator( name=\"Discriminator\"):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, 3], batch_size=BATCH_SIZE, name=\"discriminator_condition_input\")\n",
        "  tar = tf.keras.layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, 3], batch_size=BATCH_SIZE, name=\"discriminator_target_input\")\n",
        "  \n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "  \n",
        "  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1, \n",
        "                                kernel_initializer=initializer, \n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "  \n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv) \n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "  \n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "  \n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "  \n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last, name=name )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gDkA05NE6QMs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0754f0b7-bf04-434e-9a79-640f0d657676"
      },
      "cell_type": "code",
      "source": [
        "@eager\n",
        "def test_discriminator():\n",
        "  global gen_output\n",
        "  discriminator = Discriminator(name=\"D\")\n",
        "  discriminator.summary()\n",
        "  disc_out = discriminator([inp[tf.newaxis,...], gen_output], training=False)\n",
        "  plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
        "  plt.colorbar()\n",
        "  plt.axis('off')\n",
        "  \n",
        "test_discriminator()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-ede4p2YELFa"
      },
      "cell_type": "markdown",
      "source": [
        "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004)."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0FMYgY_mPfTi"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the loss functions and the optimizer\n",
        "\n",
        "* **Discriminator loss**\n",
        "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
        "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n",
        "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n",
        "  * Then the total_loss is the sum of real_loss and the generated_loss\n",
        "  \n",
        "* **Generator loss**\n",
        "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
        "  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
        "  * This allows the generated image to become structurally similar to the target image.\n",
        "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cyhxTuvJyIHV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LAMBDA = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q1Xbz5OaLj5C",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wkMNfBWlT-PV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "  \n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "90BIcCKcDMxz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "  \n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iWCn_PVdEJZ7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aKUZnDiqQrAh"
      },
      "cell_type": "markdown",
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WJnftd5sQsv6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Rw1fkAczTQYh"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "* We start by iterating over the dataset\n",
        "* The generator gets the input image and we get a generated output.\n",
        "* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n",
        "* Next, we calculate the generator and the discriminator loss.\n",
        "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
        "* This entire procedure is shown in the images below.\n",
        "\n",
        "![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/images/dis.png?raw=1)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/images/gen.png?raw=1)\n",
        "\n",
        "## Generate Images\n",
        "\n",
        "* After training, its time to generate some images!\n",
        "* We pass images from the test dataset to the generator.\n",
        "* The generator will then translate the input image into the output we expect.\n",
        "* Last step is to plot the predictions and **voila!**"
      ]
    },
    {
      "metadata": {
        "id": "xTeCDYt7kttH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output, Image, display, HTML\n",
        "\n",
        "# Helper functions for TF Graph visualization\n",
        "def strip_consts(graph_def, max_const_size=32):\n",
        "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
        "    strip_def = tf.GraphDef()\n",
        "    for n0 in graph_def.node:\n",
        "        n = strip_def.node.add() \n",
        "        n.MergeFrom(n0)\n",
        "        if n.op == 'Const':\n",
        "            tensor = n.attr['value'].tensor\n",
        "            size = len(tensor.tensor_content)\n",
        "            if size > max_const_size:\n",
        "                tensor.tensor_content = bytes(\"\"%size, 'utf-8')\n",
        "    return strip_def\n",
        "  \n",
        "def rename_nodes(graph_def, rename_func):\n",
        "    res_def = tf.GraphDef()\n",
        "    for n0 in graph_def.node:\n",
        "        n = res_def.node.add() \n",
        "        n.MergeFrom(n0)\n",
        "        n.name = rename_func(n.name)\n",
        "        for i, s in enumerate(n.input):\n",
        "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
        "    return res_def\n",
        "  \n",
        "def show_graph(graph_def=None, max_const_size=32):\n",
        "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
        "    \n",
        "    # If no input graph is given, get the default graph\n",
        "    if graph_def is None:\n",
        "        graph_def = tf.get_default_graph().as_graph_def()\n",
        "        \n",
        "    if hasattr(graph_def, 'as_graph_def'):\n",
        "        graph_def = graph_def.as_graph_def()\n",
        "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
        "    code = \"\"\"\n",
        "        <script>\n",
        "          function load() {{\n",
        "            document.getElementById(\"{id}\").pbtxt = {data};\n",
        "          }}\n",
        "        </script>\n",
        "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
        "        <div style=\"height:600px\">\n",
        "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
        "        </div>\n",
        "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
        "\n",
        "    iframe = \"\"\"\n",
        "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
        "    \"\"\".format(code.replace('\"', '&quot;'))\n",
        "    display(HTML(iframe))\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NS2GWywBbAWo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RmdVsmvhPxyy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "  # the training=True is intentional here since\n",
        "  # we want the batch statistics while running the model\n",
        "  # on the test dataset. If we use training=False, we will get \n",
        "  # the accumulated statistics learned from the training dataset\n",
        "  # (which we don't want)\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15,15))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "olnBhR_KpD5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3090
        },
        "outputId": "94665ddb-7cae-46d0-bf82-710b6e3bb79c"
      },
      "cell_type": "code",
      "source": [
        "if tf.executing_eagerly():\n",
        "  print(\"eager\")\n",
        "else:\n",
        "  print(\"static\")\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "   \n",
        "train_dataset=create_train_dataset()    \n",
        "test_dataset=create_test_dataset()\n",
        "\n",
        "\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "print(\"generator: \",generator)\n",
        "print(\"discriminator: \",discriminator)\n",
        "\n",
        "#generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "#discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "generator_optimizer = tf.train.AdamOptimizer(learning_rate=2e-4, beta1=0.5, beta2=0.999, epsilon=1e-08, name=\"generator_optimizer\")\n",
        "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.999, epsilon=1e-08, name=\"discriminator_optimizer\")\n",
        "\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "\n",
        "def train_op(iterator):\n",
        "    \n",
        "    with tf.name_scope(\"train_op\"):\n",
        "\n",
        "      input_image, target = iterator.get_next()\n",
        "\n",
        "      inp = input_image\n",
        "      tar = target\n",
        "\n",
        "      gen_output = generator(inp)\n",
        "      disc_gen_output = discriminator([inp, gen_output])\n",
        "      disc_real_output = discriminator([inp, tar])\n",
        "\n",
        "      def log_eps(i):\n",
        "          return tf.log(i+1e-11)\n",
        "\n",
        "      dloss = - tf.reduce_mean(log_eps(1-disc_gen_output) + .1 * log_eps(1-disc_real_output) + .9 * log_eps(disc_real_output))\n",
        "      gloss = - tf.reduce_mean(log_eps(disc_gen_output))\n",
        "\n",
        "      #lr,b1 = 1e-4,.2 # otherwise won't converge.\n",
        "      #optimizer = Adam(lr,beta1=b1)\n",
        "\n",
        "\n",
        "      grad_loss_wd = discriminator_optimizer.compute_gradients(dloss, discriminator.trainable_weights)\n",
        "      update_wd = discriminator_optimizer.apply_gradients(grad_loss_wd)\n",
        "\n",
        "      #generator_optimizer\n",
        "      grad_loss_wg = generator_optimizer.compute_gradients(gloss, generator.trainable_weights)\n",
        "      update_wg = generator_optimizer.apply_gradients(grad_loss_wg)\n",
        "\n",
        "      def get_internal_updates(model):\n",
        "          # get all internal update ops (like moving averages) of a model\n",
        "          inbound_nodes = model.inbound_nodes\n",
        "          input_tensors = []\n",
        "          for ibn in inbound_nodes:\n",
        "              input_tensors += ibn.input_tensors\n",
        "          updates = [model.get_updates_for(i) for i in input_tensors]\n",
        "          return updates\n",
        "\n",
        "      other_parameter_updates = [get_internal_updates(m) for m in [discriminator, generator]]\n",
        "\n",
        "      print('other_parameter_updates for the models:')\n",
        "      pprint(other_parameter_updates)\n",
        "\n",
        "      train_step = [update_wd, update_wg]#, other_parameter_updates]\n",
        "      losses = [dloss, gloss]\n",
        "      data = [input_image, target, gen_output]\n",
        "\n",
        "      #from tf.keras import backend as K\n",
        "      #print K.learning_phase()\n",
        "      #learning_phase = K.learning_phase()\n",
        "\n",
        "    return train_step, losses, data\n",
        "\n",
        "\n",
        "def trainX(dataset, epochs):  \n",
        "  \n",
        "  steps_per_epoch = 10\n",
        "  \n",
        "  iterator = dataset.make_initializable_iterator()\n",
        "  \n",
        "  train_step, losses, data = train_op(iterator)\n",
        "\n",
        "  \n",
        "\n",
        "  sess = tf.Session()\n",
        "  sess.as_default()\n",
        "  \n",
        "  sess.run([iterator.initializer, tf.global_variables_initializer()])\n",
        "\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    print('Epoch', epoch)\n",
        "    #progbar = tf.keras.utils.Progbar(target=steps_per_epoch, stateful_metrics='loss')\n",
        "    \n",
        "    for step in range(steps_per_epoch):  \n",
        "      #train_result, loss_values, images = \n",
        "      sess.run([train_step, losses, data])\n",
        "              \n",
        "    #clear_output(wait=True)\n",
        "    #for inp, tar in test_dataset.take(1):\n",
        "    #  generate_images(generator, inp, tar)\n",
        "          \n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix, session=sess)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))\n",
        "  sess.close()\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "trainX(train_dataset, EPOCHS)\n",
        "\n",
        "show_graph()\n",
        "\n",
        "input(\"Press Enter to continue...\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "static\n",
            "generator:  <tensorflow.python.keras.engine.training.Model object at 0x7f70c0536748>\n",
            "discriminator:  <tensorflow.python.keras.engine.training.Model object at 0x7f70abf4d6d8>\n",
            "other_parameter_updates for the models:\n",
            "[[[<tf.Operation 'batch_normalization_v1_16/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_17/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_17/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'batch_normalization_v1_16/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_16/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_16/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>],\n",
            "  [<tf.Operation 'batch_normalization_v1_16/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_17/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_17/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'batch_normalization_v1_16/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_16/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_16/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>]],\n",
            " [[<tf.Operation 'sequential_6/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_8/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_10/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_1/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_12/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_14/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_3/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_9/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_13/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_2/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_11/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_5/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_1/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_2/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_7/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_10/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_9/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_12/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_11/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_4/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_13/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_5/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_6/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_8/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_14/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_4/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_7/AssignMovingAvg/AssignSubVariableOp' type=AssignSubVariableOp>,\n",
            "   <tf.Operation 'sequential_3/AssignMovingAvg_1/AssignSubVariableOp' type=AssignSubVariableOp>]]]\n",
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'generator_condition_input' with dtype float and shape [8,256,256,3]\n\t [[{{node generator_condition_input}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-671e7fdc9320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0mtrainX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mshow_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-671e7fdc9320>\u001b[0m in \u001b[0;36mtrainX\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0;31m#train_result, loss_values, images =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m#clear_output(wait=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'generator_condition_input' with dtype float and shape [8,256,256,3]\n\t [[node generator_condition_input (defined at <ipython-input-23-9fb73ae9f38f>:32) ]]\n\nCaused by op 'generator_condition_input', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-35-671e7fdc9320>\", line 12, in <module>\n    generator = Generator()\n  File \"<ipython-input-23-9fb73ae9f38f>\", line 32, in Generator\n    inputs = tf.keras.layers.Input(shape=[IMG_WIDTH,IMG_HEIGHT,3], batch_size=BATCH_SIZE, name=\"generator_condition_input\")\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_layer.py\", line 231, in Input\n    input_tensor=tensor)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_layer.py\", line 107, in __init__\n    name=self.name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 876, in placeholder\n    x = array_ops.placeholder(dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 2077, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 5791, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'generator_condition_input' with dtype float and shape [8,256,256,3]\n\t [[node generator_condition_input (defined at <ipython-input-23-9fb73ae9f38f>:32) ]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "yiJQvtiZpDu2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KBKUV2sKXDbY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@tf.function\n",
        "def train_step(input_image, target, sess):\n",
        "   \n",
        "  #print(input_image)\n",
        "  #print(target)\n",
        "  \n",
        "  di =  [input_image, target]\n",
        "    \n",
        "  #if tf.executing_eagerly():\n",
        "     # di = tf.concat(input_image, target)\n",
        "  #else:\n",
        "  #    print(tf.shape(input_image).eval(session=sess), tf.shape(target).eval(session=sess))\n",
        "  #    di = sess.run(tf.concat([input_image, target], axis=3)) \n",
        "\n",
        "  #print(di)\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        \n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    #dg = tf.concat([input_image, gen_output], axis=3).eval(session=sess)\n",
        "    dg = [input_image, gen_output]\n",
        "    disc_generated_output = discriminator(dg, training=True)\n",
        "    gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "\n",
        "    disc_real_output = discriminator(di, training=True)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2M7LmLtGEMQJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):  \n",
        "  \n",
        "  steps_per_epoch = 10\n",
        "  sess = None\n",
        "  use_tape = 0\n",
        "  use_keras_train = 0\n",
        "  \n",
        "  if tf.executing_eagerly():\n",
        "    print(\"eager\")\n",
        "    iterator = dataset.make_one_shot_iterator()    \n",
        "  else:\n",
        "    print(\"static\")\n",
        "    iterator = dataset.make_initializable_iterator()\n",
        "    sess = tf.Session()\n",
        "    sess.as_default()\n",
        "    sess.run(iterator.initializer)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    a, b = sess.run(iterator.get_next())\n",
        "  \n",
        "    \n",
        "    if not use_tape:\n",
        "      if use_keras_train:\n",
        "        #discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')\n",
        "        #generator.compile(optimizer=generator_optimizer, loss='binary_crossentropy')\n",
        "\n",
        "        #shape = discriminator.get_input_shape_at(0)[1:]\n",
        "        #fake_input, real_input = Input(shape), Input(shape)\n",
        "        #discriminator2batch = Model([fake_input, real_input], [discriminator(fake_input), discriminator(real_input)])\n",
        "        #discriminator.trainable = True\n",
        "        #discriminator2batch.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "\n",
        "        #input_image = Input(shape=[IMG_WIDTH,IMG_HEIGHT,3], batch_size=BATCH_SIZE)\n",
        "        #gen_output = generator(input_image)\n",
        "        #disc_input = tf.keras.layers.concatenate([input_image, gen_output]) \n",
        "        #disc_gen_output = discriminator(disc_input)\n",
        "        #generator_discriminator =  Model(inputs=input_image, outputs=[gen_output, disc_gen_output])\n",
        "        #generator_discriminator.summary()\n",
        "        #discriminator.trainable=False\n",
        "        #generator_discriminator.compile(optimizer=generator_optimizer, loss=['mae', 'binary_crossentropy'], loss_weights=[10, 1])\n",
        "        print(\"K\")\n",
        "      else:\n",
        "        \n",
        "        #input_image = Input(shape=generator.input_shape[1:])\n",
        "        #real_input = Input(shape=discriminator.input_shape[1:])\n",
        "        \n",
        "        input_image, target = iterator.get_next()\n",
        "        \n",
        "        \n",
        "        gen_output = generator.predict(input_image,batch_size=BATCH_SIZE, verbose=1,steps=1)\n",
        "        disc_gen_output = discriminator.predict([input_image,gen_output],batch_size=BATCH_SIZE, verbose=1,steps=1)\n",
        "        disc_real_output = discriminator.predict([input_image,target],batch_size=BATCH_SIZE, verbose=1,steps=1)\n",
        "        \n",
        "        \n",
        "             \n",
        "        \n",
        "        #inp = tf.keras.layers.InputLayer(input_shape=[IMG_WIDTH, IMG_HEIGHT, 3], name='train_input_image', batch_size=BATCH_SIZE, input_tensor=input_image)\n",
        "        #tar = tf.keras.layers.InputLayer(input_shape=[IMG_WIDTH, IMG_HEIGHT, 3], name='train_target_image', batch_size=BATCH_SIZE, input_tensor=target)\n",
        "  \n",
        "        #gen_output = generator(inp)\n",
        "        #disc_gen_output = discriminator([inp,gen_output])\n",
        "        #disc_real_output = discriminator([inp,tar])\n",
        "\n",
        "        def log_eps(i):\n",
        "            return tf.log(i+1e-11)\n",
        "\n",
        "        dloss = - tf.reduce_mean(log_eps(1-disc_gen_output) + .1 * log_eps(1-disc_real_output) + .9 * log_eps(disc_real_output))\n",
        "        gloss = - tf.reduce_mean(log_eps(disc_gen_output))\n",
        "\n",
        "        Adam = tf.train.AdamOptimizer\n",
        "\n",
        "        lr,b1 = 1e-4,.2\n",
        "        optimizer = Adam(lr,beta1=b1)\n",
        "\n",
        "        grad_loss_wd = optimizer.compute_gradients(dloss, discriminator.trainable_weights)\n",
        "        update_wd = optimizer.apply_gradients(grad_loss_wd)\n",
        "\n",
        "        grad_loss_wg = optimizer.compute_gradients(gloss, generator.trainable_weights)\n",
        "        update_wg = optimizer.apply_gradients(grad_loss_wg)\n",
        "\n",
        "        def get_internal_updates(model):\n",
        "            # get all internal update ops (like moving averages) of a model\n",
        "            inbound_nodes = model.inbound_nodes\n",
        "            input_tensors = []\n",
        "            for ibn in inbound_nodes:\n",
        "                input_tensors+= ibn.input_tensors\n",
        "            updates = [model.get_updates_for(i) for i in input_tensors]\n",
        "            return updates\n",
        "\n",
        "        other_parameter_updates = [get_internal_updates(m) for m in [discriminator,generator]]\n",
        "        # those updates includes batch norm.\n",
        "\n",
        "        print('other_parameter_updates for the models:')\n",
        "        print(other_parameter_updates)\n",
        "\n",
        "        train_step = [update_wd, update_wg, other_parameter_updates]\n",
        "        losses = [dloss, gloss]\n",
        "        data = [input_image, target, gen_output]\n",
        "        \n",
        "        #from tf.keras import backend as K\n",
        "        #print K.learning_phase()\n",
        "        #learning_phase = K.learning_phase()\n",
        "    \n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    print('Epoch', epoch)\n",
        "    progbar = tf.keras.utils.Progbar(target=steps_per_epoch, stateful_metrics='loss')\n",
        "    \n",
        "    for step in range(steps_per_epoch):  \n",
        "    #for input_image, target in dataset:\n",
        "    \n",
        "\n",
        "        if tf.executing_eagerly():\n",
        "          input_image, target = iterator.get_next()\n",
        "          train_step(input_image, target, sess)\n",
        "        else:\n",
        "          if use_tape:\n",
        "            input_image, target = sess.run(iterator.get_next())\n",
        "            train_step(input_image, target, sess)\n",
        "          else:\n",
        "            if use_keras_train:\n",
        "              #input_image, target = sess.run(iterator.get_next())\n",
        "              #y = np.ones((BATCH_SIZE, 1))\n",
        "              #g_loss = generator_discriminator.train_on_batch(input_image, y)\n",
        "              #gen_img = generator.predict(input_image)\n",
        "              #gen_y   = np.zeros((nbatch, 1))\n",
        "              #real_y  = np.ones((nbatch, 1))\n",
        "              #d_loss = discriminator2batch.train_on_batch([gen_img, real_img], [gen_y, real_y])\n",
        "              print(\"K\")\n",
        "            else:         \n",
        "              \n",
        "              train_result, loss_values, images = sess.run([train_step, losses, data])\n",
        "              \n",
        "\n",
        "    clear_output(wait=True)\n",
        "    for inp, tar in test_dataset.take(1):\n",
        "      generate_images(generator, inp, tar)\n",
        "          \n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))\n",
        "    \n",
        "     \n",
        "  if tf.executing_eagerly():\n",
        "    print(\"eager\")\n",
        "  else:\n",
        "    sess.close()\n",
        "    print(\"static\")\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "a1zZmKmvOH85",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kz80bY3aQ1VZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HSSm4kfvJiqv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls {checkpoint_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4t4x69adQ5xb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1RGysMU_BZhx"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing on the entire test dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KUgSnmy2nqSP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run the trained model on the entire test dataset\n",
        "for inp, tar in test_dataset:\n",
        "  generate_images(generator, inp, tar)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}